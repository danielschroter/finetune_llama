{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch\n",
    "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
    "!pip install -q datasets #huggingface datasets to download and manipulate datasets\n",
    "!pip install -q peft #Parameter efficient finetuning - for qLora Finetuning\n",
    "!pip install -q bitsandbytes #For Model weights quantisation\n",
    "!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n",
    "!pip install -q wandb -U #Used to monitor the model score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dschr\\anaconda3\\envs\\gpt-use-cases\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer # For supervised finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# Log in to HF Hub\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <th>Self-direction: action</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Conformity: rules</th>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <th>Humility</th>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <th>Universalism: concern</th>\n",
       "      <th>Universalism: nature</th>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <th>Universalism: objectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4568</th>\n",
       "      <td>A30388</td>\n",
       "      <td>We should adopt an austerity regime</td>\n",
       "      <td>against</td>\n",
       "      <td>we should not support the austerity regime bec...</td>\n",
       "      <td>we should not support the austerity regime bec...</td>\n",
       "      <td>['Security: personal', 'Benevolence: caring', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>A19198</td>\n",
       "      <td>We should legalize cannabis</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>in many cases cannabis has wonderful health re...</td>\n",
       "      <td>in many cases cannabis has wonderful health re...</td>\n",
       "      <td>['Self-direction: action', 'Security: personal...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>A18079</td>\n",
       "      <td>We should ban cosmetic surgery</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>the long term effects of new procedures have n...</td>\n",
       "      <td>the long term effects of new procedures have n...</td>\n",
       "      <td>['Security: personal', 'Benevolence: caring', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>E01075</td>\n",
       "      <td>Devices with batteries should be designed so t...</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>Having to replace an entire mobile phone (or t...</td>\n",
       "      <td>Having to replace an entire mobile phone (or t...</td>\n",
       "      <td>['Security: personal', 'Universalism: nature']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>A22268</td>\n",
       "      <td>We should legalize cannabis</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should legalize cannabis because it is harm...</td>\n",
       "      <td>we should legalize cannabis because it is harm...</td>\n",
       "      <td>['Self-direction: action', 'Hedonism', 'Humili...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Argument ID                                         Conclusion  \\\n",
       "4568      A30388                We should adopt an austerity regime   \n",
       "1142      A19198                        We should legalize cannabis   \n",
       "743       A18079                     We should ban cosmetic surgery   \n",
       "4824      E01075  Devices with batteries should be designed so t...   \n",
       "2168      A22268                        We should legalize cannabis   \n",
       "\n",
       "           Stance                                            Premise  \\\n",
       "4568      against  we should not support the austerity regime bec...   \n",
       "1142  in favor of  in many cases cannabis has wonderful health re...   \n",
       "743   in favor of  the long term effects of new procedures have n...   \n",
       "4824  in favor of  Having to replace an entire mobile phone (or t...   \n",
       "2168  in favor of  we should legalize cannabis because it is harm...   \n",
       "\n",
       "                                                   text  \\\n",
       "4568  we should not support the austerity regime bec...   \n",
       "1142  in many cases cannabis has wonderful health re...   \n",
       "743   the long term effects of new procedures have n...   \n",
       "4824  Having to replace an entire mobile phone (or t...   \n",
       "2168  we should legalize cannabis because it is harm...   \n",
       "\n",
       "                                               category  \\\n",
       "4568  ['Security: personal', 'Benevolence: caring', ...   \n",
       "1142  ['Self-direction: action', 'Security: personal...   \n",
       "743   ['Security: personal', 'Benevolence: caring', ...   \n",
       "4824     ['Security: personal', 'Universalism: nature']   \n",
       "2168  ['Self-direction: action', 'Hedonism', 'Humili...   \n",
       "\n",
       "      Self-direction: thought  Self-direction: action  Stimulation  Hedonism  \\\n",
       "4568                        0                       0            0         0   \n",
       "1142                        0                       1            0         0   \n",
       "743                         0                       0            0         0   \n",
       "4824                        0                       0            0         0   \n",
       "2168                        0                       1            0         1   \n",
       "\n",
       "      ...  Tradition  Conformity: rules  Conformity: interpersonal  Humility  \\\n",
       "4568  ...          0                  0                          0         0   \n",
       "1142  ...          0                  0                          0         0   \n",
       "743   ...          0                  0                          0         0   \n",
       "4824  ...          0                  0                          0         0   \n",
       "2168  ...          0                  0                          0         1   \n",
       "\n",
       "      Benevolence: caring  Benevolence: dependability  Universalism: concern  \\\n",
       "4568                    1                           0                      1   \n",
       "1142                    1                           0                      0   \n",
       "743                     1                           1                      0   \n",
       "4824                    0                           0                      0   \n",
       "2168                    0                           0                      0   \n",
       "\n",
       "      Universalism: nature  Universalism: tolerance  Universalism: objectivity  \n",
       "4568                     0                        0                          1  \n",
       "1142                     1                        0                          0  \n",
       "743                      0                        0                          0  \n",
       "4824                     1                        0                          0  \n",
       "2168                     0                        0                          0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_training_full.csv', header=0, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_format(row):\n",
    "    sentence1 = row['text']\n",
    "    sentence2 = row['category']\n",
    "    prompt = \"\"\"Below is an instruction that describes a task paired with input that provides further context. Write a response that appropriately completes the request.\"\"\"\n",
    "    instruction = \"\"\"Given the following input, your job is to label it with one or multiple underlying human values according to Schwartz Value Theory. The human values are implict present in the argument. \\\n",
    "        All available human values are: \\\n",
    "        \"Self-direction: thought, Self-direction: action, Stimulation, Hedonism, Achievement, Power: dominance, Power: resources, Face, Security: personal, Security: societal, Tradition, Conformity: rules, Conformity: interpersonal, Humility, Benevolence: caring, Benevolence: dependability, Universalism: concern, Universalism: nature, Universalism: tolerance, Universalism: objectivity\" . Use only these labels in your answer.\"\"\"\n",
    "    input = str(sentence1)\n",
    "    response = f\"\"\"'human values': '{sentence2}'\"\"\"\n",
    "    if len(input.strip()) == 0:  #  prompt + 2 new lines + ###instruction + new line + input + new line + ###response\n",
    "        text = f\"\"\"<s>[INST]  {instruction} [/INST] \\\\n {response} </s>\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"<s>[INST]  {instruction} \\n Input: {input} [/INST] \\n {response} </s>\"\"\"\n",
    "\n",
    "    \n",
    "    # we need 4 columns for to train: instruction, input, output, text\n",
    "    return pd.Series([instruction, input, response, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(convert_to_format, axis=1)\n",
    "df.columns = ['instruction', 'input', 'output', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_instruct_format_mistral.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]  Given the following input, your job is to label it with one or multiple underlying human values according to Schwartz Value Theory. The human values are implict present in the argument.         All available human values are:         \"Self-direction: thought, Self-direction: action, Stimulation, Hedonism, Achievement, Power: dominance, Power: resources, Face, Security: personal, Security: societal, Tradition, Conformity: rules, Conformity: interpersonal, Humility, Benevolence: caring, Benevolence: dependability, Universalism: concern, Universalism: nature, Universalism: tolerance, Universalism: objectivity\" . Use only these labels in your answer. \n",
      " Input: we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same. in favor of We should ban human cloning [/INST] \n",
      " 'human values': '['Security: societal']' </s>\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/data_instruct_format_mistral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'text', '__index_level_0__'],\n",
       "    num_rows: 6876\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.remove_columns(\"__index_level_0__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name =\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# model_name = \"abhishek/llama-2-7b-hf-small-shards\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"mistrala-7b-qlora-value-detector\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 50\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "print(compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100, # the number of training steps the model will take\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",  # this is the text column in dataset \n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "import gc\n",
    "del base_model\n",
    "gc.collect()\n",
    "\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # PyTorch thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "lora_config = LoraConfig.from_pretrained(new_model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.push_to_hub(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "sample = train_dataset [randrange(len(train_dataset ))]\n",
    "\n",
    "prompt = f\"\"\"<s>\n",
    "{sample['instruction']}\n",
    "{sample['input']}\n",
    "[INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = merged_model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"\\nGenerated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"\\nGround truth:\\n{sample['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "adapters_name = \"danschr/llama-2-qlora-value-detector\"\n",
    "\n",
    "print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "m = PeftModel.from_pretrained(m, adapters_name)\n",
    "m = m.merge_and_unload()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "inputs = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "outputs = m.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=m, tokenizer=tokenizer, max_length=300)\n",
    "results = pipe(input_sentence)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r llama-2-qlora-value-detector.zip results llama-2-qlora-value-detector\n",
    "!mv llama-2-qlora-value-detector.zip /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/llama-2-qlora-value-detector.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Instruction:\n",
    "Given the following sentence, your job is to label it with one or multiple underlying human values according to Schwartz Value Theory. They human values are implict present in the argument. The answer should have the following format: Values: [value1, value2, ... ]. All available human values are: \"Self-direction: thought, Self-direction: action, Stimulation, Hedonism, Achievement, Power: dominance, Power: resources, Face, Security: personal, Security: societal, Tradition, Conformity: rules, Conformity: interpersonal, Humility, Benevolence: caring, Benevolence: dependability, Universalism: concern, Universalism: nature, Universalism: tolerance, Universalism: objectivity\" . Use only these labels in your answer.\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "sentence = dataset[10][\"input\"]\n",
    "input_sentence = prompt_template.format(sentence.strip())\n",
    "print(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def format_results(s):\n",
    "  pattern = r'```json\\n(.*?)\\n```'\n",
    "\n",
    "  # Find all occurrences of JSON objects in the string\n",
    "  json_matches = re.findall(pattern, s, re.DOTALL)\n",
    "  if not json_matches:\n",
    "    # try to find 2nd pattern\n",
    "    pattern = r'\\{.*?\"sentence\":.*?\"negation\":.*?\\}'\n",
    "    json_matches = re.findall(pattern, s)\n",
    "\n",
    "  # Return the first JSON object found, or None if no match is found\n",
    "  return json.loads(json_matches[0]) if json_matches else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-use-cases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
